---
documentclass: article
fontsize: 12pt
output:
  bookdown::pdf_document2: 
    toc: false
    latex_engine: xelatex
    includes:
      in_header: preamble.sty
      before_body: titlepage.sty
---

```{r setup, include=FALSE}
options(knitr.kable.NA = '')
#notes on fonts, font size, line spacing https://bookdown.org/yihui/rmarkdown-cookbook/latex-variables.html
knitr::opts_chunk$set(echo = FALSE, fig.pos = "H", out.extra = "")

library(here)
library(tidyverse)
library(cowplot)
library(kableExtra)
library(multcomp)
library(DHARMa) # to simulate residuals
library(MASS)
library(pscl) #zero-inflated poisson
#read the data
library(vcd) #for the rootogram
library(gtools)#for stars.pval
library(countreg)

data <- read.delim(file = sprintf("%s/data/homicidevictim.txt",here())) %>% 
  mutate(race = as.factor(race))

summ <- data %>% group_by(race) %>%
  summarize(nb = n(),
            mean = round(mean(resp), digits = 2),
            median = round(median(resp), digits = 2),
            variance = round(var(resp), digits = 2))
```

# Introduction

This report investigates the link between a person's race and the number of homicide victims that person knows. `r nrow(data)` people were asked how many homicide victims they know. The raw data is analyzed in Section \@ref(EDA) after which several statistical models are explored in Section \@ref(MET). Lastly, Section \@ref(CON) concludes the report.

# Data Exploration{#EDA}

In total, `r nrow(data)` people were asked how many homicide victims they knew. Figure \@ref(fig:Response) shows the absolute and relative number of people for each race that knew 0, 1, 2, 3, 4, 5, or 6 homicide victims. There are a lot more white participants in the study (`r summ[2,'nb']` (`r round(summ[2,'nb']/nrow(data)*100, digits=2)`%) white versus `r summ[1,'nb']` (`r round(summ[1,'nb']/nrow(data)*100,digits=2)`%) black people were questioned) and the relative frequencies show that black people know more homicide victims on average (mean of number of known homicide victims for white respondents is `r summ[2,'mean']` with a variance of `r summ[2,'variance']`, when for the black respondents this mean is `r summ[1,'mean']` with a variance of `r summ[1,'variance']`).

```{r Response, include=TRUE, message = FALSE, fig.cap = "Absolute (A) and relative (B) number of people in each race and response group (number of homicide victims the person knows). The mean is indicated with a vertical line.", fig.height = 4, fig.width = 10}
p1 <- data %>% group_by(resp, race) %>%
  summarize(n = n()) %>% 
  complete(resp, race, fill = list(n = 0)) %>%
  distinct() %>%
  ungroup() %>%
  ggplot() + 
  geom_bar(aes(x = resp, fill = race, y = n), 
                 stat = "identity", position = "dodge", alpha = 0.5) +
  geom_text(aes(x = resp - 0.2*(race == "black") + 0.2*(race == "white"), 
                y = (n + 10), 
                label = n)) +
  geom_vline(data = summ, aes(color = race, xintercept = mean)) +
  ylab("Number of people in each race") + 
  xlab("Number of known homicide victims") +
  theme_bw() + theme(legend.position = c(0.8, 0.8))
p2 <- data %>% group_by(resp, race) %>%
  summarize(n = n()) %>% ungroup() %>% group_by(race) %>%
  mutate(perc = n/sum(n)) %>%
  complete(resp, race, fill = list(n = 0, perc = 0)) %>%
  distinct() %>%
  ungroup() %>%
  ggplot() + 
  geom_bar(aes(x = resp, fill = race, y = perc), 
                 stat = "identity", position = "dodge", alpha = 0.5) +
  geom_text(aes(x = resp - 0.2*(race == "black") + 0.2*(race == "white"), 
                y = (perc + 0.02), 
                label = sprintf("%d%%",round(perc*100)))) +
  geom_vline(data = summ,aes(color = race, xintercept = mean)) + 
  theme_bw() + theme(legend.position = c(0.8, 0.8)) + 
  scale_y_continuous(labels = scales::percent) +
  ylab("Percentage of people in each race") + 
  xlab("Number of known homicide victims")
plot_grid(p1, p2, labels = c('A', 'B'), label_size = 12)
```

```{r Responsetab, include=FALSE, message = FALSE}
data %>% group_by(race, resp) %>%
  summarize(n = n()) %>% 
  ungroup() %>% 
  group_by(race) %>%
  mutate(perc = n/sum(n)) %>%
  complete(race, resp, fill = list(n = 0, perc = 0)) %>%
  distinct() %>%
  arrange(race, resp) %>%
  mutate(perc = sprintf("%.2f%%",round(100*perc, 2))) %>%
  ungroup() %>%
  kable(booktabs = TRUE, 
        caption = "Summary data.", 
        linesep = "",
        col.names = str_wrap(c('Race', 'Response', 'Number of people',
                              'Percentage of people within each race'),
                             width = 20 )) %>%
  kable_styling(font_size = 8)
```


# Methodology & results {#MET}

## Poisson model

```{r poissonmodel,include=FALSE, message = FALSE}
mod.poisson <- glm(resp ~ race, family = poisson, data = data)
sum.mod.poisson <- summary(mod.poisson)

sum.mod.poisson$coefficients %>%
  kable(booktabs = T, 
        caption = "Poisson model.") %>% kable_styling()
#the following function calculates risk ratios with confidence interval. Source: https://rpubs.com/kaz_yos/poisson
glm.RR <- function(GLM.RESULT, digits = 2) {

    if (GLM.RESULT$family$family == "binomial") {
        LABEL <- "OR"
    } else if (GLM.RESULT$family$family == "poisson") {
        LABEL <- "RR"
    } else {
        stop("Not logistic or Poisson model")
    }

    COEF      <- stats::coef(GLM.RESULT)
    CONFINT   <- stats::confint(GLM.RESULT)
    TABLE     <- cbind(coef=COEF, CONFINT)
    TABLE.EXP <- round(exp(TABLE), digits)

    colnames(TABLE.EXP)[1] <- LABEL

    TABLE.EXP
}
poissonrr <- glm.RR(mod.poisson)
intercept.poisson <- sum.mod.poisson$coefficients[1,1]
intercept.std.poisson <- sum.mod.poisson$coefficients[1,2]
race.poisson <- sum.mod.poisson$coefficients[2,1]
race.std.poisson <- sum.mod.poisson$coefficients[2,2]
aic.poisson <- AIC(mod.poisson)
meanblack.poisson <- exp(intercept.poisson)
meanwhite.poisson <- exp(intercept.poisson + race.poisson)
```

Since the number of homicide victims a person knows is count data, a Poisson model is first applied (Table \@ref(tab:comparemodels) in the appendix). The model shows that, on average, white people know less homicide victims than black people. Risk ratios in Table \@ref(tab:poissonrr) show that the number of homicide victims known by white people is `r poissonrr[2,1]` times that of black people.
To calculate the means for each level of covariate, the exponential transformation is being used: $e^{\textrm{`r round(intercept.poisson,digits = 2)`}} = `r round(exp(intercept.poisson), digits = 2)`$ and $e^{\textrm{`r round(intercept.poisson, digits = 2)`} \textrm{`r round(race.poisson, digits = 2)`}} = `r round(exp(intercept.poisson + race.poisson), digits = 2)`$ for white individuals. The ratio of the mean responses is `r round(exp(sum.mod.poisson$coefficients[1,1] )/exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1]), digits = 2)` (black/white) and `r round(exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1])/exp(sum.mod.poisson$coefficients[1,1]), digits = 2)` (white/black). Meaning that, on average, a black person knows `r round(exp(sum.mod.poisson$coefficients[1,1] )/exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1]), digits=2)` times more homicide victims than a white person.

```{r poissonrr, include=TRUE, message = FALSE}
kable(poissonrr, 
        caption = "Poisson risk ratios.") %>%
  kable_styling(font_size = 8)
```

Figure \@ref(fig:comparemodels3) in the appendix compares the true data with the predicted probabilities.
Although the model is very accurate with respect to the mean, the variance is larger in reality than in the Poisson model.
Furthermore, there may be some zero-inflation, especially for the black population.

```{r poissonpredict, include=FALSE, message = FALSE, fig.cap = "True and predicted probabilities of people knowing a certain number of homicide victims, for each race.", fig.width = 10, fig.height = 6}
ratewhite <- exp(sum.mod.poisson$coefficients[1,1] +  sum.mod.poisson$coefficients[2,1])
rateblack <- exp(sum.mod.poisson$coefficients[1,1])

data %>% group_by(resp, race) %>%
  summarize(n = n()) %>% ungroup() %>% group_by(race) %>%
  mutate(perc = n/sum(n)) %>%
  complete(resp, race, fill = list(n = 0, perc = 0)) %>%
  distinct() %>%
  ungroup() %>%
  mutate(rate = ifelse(race=="black", rateblack,ratewhite),
         prediction = rate^(resp)/factorial(resp)*exp(-1*rate)) %>%
  pivot_longer(cols = c(perc,prediction), names_to = "type", 
               values_to = "value") %>%
  ggplot() + 
  geom_bar(aes(x = resp, y = value,fill = type), stat = "identity", 
           position = "dodge") +
  geom_text(aes(x = resp - 0.2*(type == "perc") + 0.2*(type == "prediction"), 
                y = (value + 0.02), 
                label = sprintf("%d%%", round(value * 100)))) +
  theme_bw() + theme(legend.position = c(0.8, 0.8)) + 
  scale_y_continuous(labels = scales::percent) +
  ylab("Percentage of predicted or true people in each race") + 
  xlab("Number of known homicide victims") +
  facet_grid(cols = vars(race)) +
  scale_fill_discrete(name = "", labels = c('True probability','Predicted probability'))
```

An important assumption in a Poisson model is that mean and variance are equal. The variance in the data is `r round(var(data$resp), digits = 3)` (`r unname(unlist(summ[1,5]))` for black respondents and `r unname(unlist(summ[2,5]))` for white respondents), while the mean is `r round(mean(data$resp),digits =2)`. Indeed, Figure \@ref(fig:GOF1) shows there is overdispersion (the real variance in red is larger than the simulated variance). 


```{r include=FALSE, message = FALSE, fig.height = 5, fig.width = 10}
sim.mod.poisson <- simulateResiduals(mod.poisson, plot = T)
```

```{r GOF1, include=TRUE, message = FALSE, results='hide', fig.cap = "Test of uniformity and dispersion", fig.width = 10, fig.height = 5}
par(mfrow=c(1,2)) 
testUniformity(sim.mod.poisson)
testDispersion(sim.mod.poisson)
```

```{r ROOT, include = TRUE, message = FALSE, fig.cap = "Histogram of DHARMa residulas and rootogram", fig.height = 5, fig.width = 10}
par(mfrow=c(1,2)) 
hist(sim.mod.poisson)
#install.packages("countreg", repos="http://R-Forge.R-project.org")
rootogram(mod.poisson, ylab = "Root Square of Frequency", main = "Rootogram")
```


```{r outlierdetection, include = FALSE, message = FALSE, warning = FALSE}
library(rgl)
library(MASS)
library(robustbase)

n <- dim(data)[1]
p <- dim(data)[2]


#DFFITS
p.dffits <- dffits(mod.poisson)
plot(p.dffits, ylab = "DFFITS")
abline(h = 2*sqrt(p/n), col = "red")

# Cook's distance
#Cook's distance measures the influence of the ith case on all n
#fitted values 
Cd <- cooks.distance(mod.poisson)
plot(Cd, ylab = "Cook's distance")
abline(h = 1, col = "red")
#usually pinpoints less outliers-> here none are >1

# DFBETAS
#The DFBETAS measure computes for each case i its influence on
#each regression coefficient beta hat
LS.dfbetas <- dfbetas(mod.poisson)
LS.dfbetas
2/sqrt(n)

#cunstructing delta Chi 2
epi <- (data$resp-mod.poisson$fitted.values)/sd(mod.poisson$fitted.values)
# Diagonal elements of hat matrix
p.influence <- influence(mod.poisson)
plot(p.influence$hat, ylab = "Diagonal elements of hat matrix")
espi <- epi/sqrt(1-p.influence$hat)
deltaChi2 <- espi^2
plot(deltaChi2)
```

The histogram of DHARMa residuals shows the presence of outliers. However, more detailed investigation using Cook's distance, DFFITS, DFBETAS, or delta Chi$^2$ showed inconsistent results on the outliers. Therefore, no observations were removed as outliers.

Kolmogorov-Smirnov test in the residuals' QQ plot indicates a borderline uniformity violation (see Figure \@ref(fig:GOF1)). However, when performing visual inspection of the histogram of DHARMa residuals, the bars do not appear to follow a uniform distribution.

The rootogram (see Figure \@ref(fig:ROOT)) shows that level 1 is overpredicted by our model, while other levels are moderately underpredicted. 

As already mentioned, the red line in Figure \@ref(fig:GOF1), which represents the observed variance, is significantly larger than what we would expect under the model. This indicates the presence of overdispersion. 

```{r outliers, include=TRUE, message = FALSE, fig.cap = "Test of outliers", fig.width = 10, fig.height = 6}
#testOutliers(sim.mod.poisson, type = "bootstrap")
```

## Negative-binomial model
 
```{r negbin, include=FALSE, message = FALSE}
mod.nb <- glm.nb(resp ~ race, data = data)
sumnb <- summary(mod.nb)
sumnb$coefficients %>%
  rbind(theta = c(mod.nb$theta, mod.nb$SE.theta, NA, NA)) %>%
  kable(booktabs = T, 
        caption = "Negative binomial model.") %>% kable_styling()
meanblack.nb <- round(exp(unlist(mod.nb$coefficients[1])), digits = 2)
thetanb <- mod.nb$theta
meanwhite.nb <- round(exp(unlist(mod.nb$coefficients[1]) + unlist(mod.nb$coefficients[2])), digits = 2)
aic.nb <- AIC(mod.nb)
intercept.nb <- sumnb$coefficients[1,1]
intercept.std.nb <- sumnb$coefficients[1,2]
race.nb <- sumnb$coefficients[2,1]
race.std.nb <- sumnb$coefficients[2,2]
varblack.nb <- unname(meanblack.nb + 1/thetanb*meanblack.nb^2)
varwhite.nb <- unname(meanwhite.nb + 1/thetanb*meanwhite.nb^2)
```

The negative binomial model assumes a quadratic relationship between the mean and the variance. The model shows similar estimated coefficients but much larger standard deviations than the Poisson model (see Table \@ref(tab:comparemodels)). The variance for each of the races can be obtained from the equation:

\vspace*{-7mm}
\begin{center}
$\sigma^2 = \mu+\frac{1}{\theta}\mu^2$, where $\mu = e^{x' \hat{\beta} }$
\end{center}
\vspace*{-4mm}

For black people, mean $\mu =$ `r unname(meanblack.nb)` and the variance is `r round(varblack.nb, digits = 2)`. For white people, $\mu =$ `r unname(meanwhite.nb)` and the variance is `r round(varwhite.nb, digits = 2)`. This shows that the variance for black people is overestimated (it was `r summ[1,5]` in reality) and the variance for white people is slightly underestimated (it was `r summ[2,5]` in reality).

## Quasi-likelihood model

The quasi-likelihood model lifts the Poisson assumption that mean and variance are equal. In general, if the mean structure is specified as $\lambda = \mu(x,\beta) = e^{x'\beta}$, then the variance is $var(y_i) = \phi\lambda$ where $\hat{\beta}$ and $\phi$ are estimated from the Pearson statistic. This model thus assumes a linear relationship between the mean and variance.

```{r quasip, include=FALSE, message = FALSE}
mod.quasip <- glm(resp ~ race, family = quasipoisson, data = data)
sum.quasip <- summary(mod.quasip)
sum.quasip$coefficients %>%
  kable(booktabs = T, 
        caption = "Quasi-likelihood model.") %>% kable_styling()

intercept.quasip <- sum.quasip$coefficients[1,1]
intercept.std.quasip <- sum.quasip$coefficients[1,2]
race.quasip <- sum.quasip$coefficients[2,1]
race.std.quasip <- sum.quasip$coefficients[2,2]
aic.quasip <- AIC(mod.quasip)
meanblack.quasip <- exp(intercept.quasip)
varblack.quasip <-  sum.quasip$dispersion * exp(intercept.quasip)
meanwhite.quasip <-  exp(intercept.quasip + race.quasip)
varwhite.quasip <-  sum.quasip$dispersion * exp(intercept.quasip + race.quasip)
```

Regression results (Table \@ref(tab:comparemodels)) show that the dispersion parameter $\phi$ is estimated to be `r round(sum.quasip$dispersion, digits = 2)`, meaning the variance is estimated to be 75\% larger than the mean (Poisson model assumes mean and variance are equal).
The variances are estimated to be `r round(exp(intercept.quasip)*sum.quasip$dispersion, digits = 2)` for black people and `r round(exp(intercept.quasip + race.quasip)*sum.quasip$dispersion, digits = 2)` for white people.

## Zero-inflated models

Lastly, a zero-inflated Poisson model and negative binomial model were tested, because the raw data shows that there are many people that know no homicide victims. 

```{r zip, include=FALSE, message = FALSE}
mod.zip <- zeroinfl(resp ~ race | race,  dist = 'poisson', data = data)
mnull <- update(mod.zip, . ~ 1)
#pchisq(2 * (logLik(mod.zip) - logLik(mnull)), df = 3, lower.tail = FALSE)
sumzip <- summary(mod.zip)
sumzip$coefficients$count %>%
  rbind(sumzip$coefficients$zero) %>%
  kable(booktabs = T, 
        caption = "Zero-inflated Poisson.") %>%
  group_rows("Count",1,2) %>%
  group_rows("Probability of zero",3,4) %>%
  kable_styling()
E2 <- resid(mod.zip, type = "pearson")
N  <- nrow(data)
p  <- length(coef(mod.zip))  
disp.zip <- sum(E2^2) / (N - p)#1.09dispersion statistic gets really close to one!
aic.zip <- AIC(mod.zip)
intercept.zip <- sumzip$coefficients$count[1,1]
intercept.std.zip <- sumzip$coefficients$count[1,2]
race.zip <- sumzip$coefficients$count[2,1]
race.std.zip <- sumzip$coefficients$count[2,2]
intercept.zero.zip <- sumzip$coefficients$zero[1,1]
intercept.zero.std.zip <- sumzip$coefficients$zero[1,2]
race.zero.zip <- sumzip$coefficients$zero[2,1]
race.zero.std.zip <- sumzip$coefficients$zero[2,2]
lambdablack.zip <- exp(intercept.zip)
lambdawhite.zip <- exp(intercept.zip + race.zip)
piblack.zip <- exp(intercept.zero.zip) /  (1 + exp(intercept.zero.zip)) #binomial model
piwhite.zip <- exp(intercept.zero.zip + race.zero.zip) / 
  (1 + exp(intercept.zero.zip + race.zero.zip)) #binomial model
#https://stats.stackexchange.com/questions/18661/mean-and-variance-of-a-zero-inflated-poisson-distribution
meanblack.zip <- (1-piblack.zip)*lambdablack.zip
varblack.zip <- (1-piblack.zip)*lambdablack.zip + piblack.zip/(1-piblack.zip)* ((1-piblack.zip)*lambdablack.zip)^2
meanwhite.zip <- (1-piwhite.zip)*lambdawhite.zip
varwhite.zip <- (1-piwhite.zip)*lambdawhite.zip + piwhite.zip/(1-piwhite.zip)* ((1-piwhite.zip)*lambdawhite.zip)^2
```

The zero-inflated Poisson (zip) model shows that white respondents are significantly more likely to know no homicide victims, and the Poisson regression coefficient for white respondents is still negative and highly significant (see Table \@ref(tab:comparemodels)). The mean and variance are calculated as: 

\vspace*{-5mm}
\begin{center}
$\mu_i = (1-\pi_i)\lambda_i$ and $\sigma^2_i = \mu_i + \frac{\pi_i}{(1-\pi_i)}\mu^2_i$, with $i \in \{white, black\},$
\end{center}
\vspace*{-5mm}

where $\lambda$ is the average rate in the count process, and $\pi$ the probability of the respondent knowing no homicide victims. This yields $\mu_{black}=$ `r round(meanblack.zip, digits=2)` and $\sigma^2_{black} =$ `r round(varblack.zip, digits=2)` for black respondents, and $\mu_{white}=$ `r round(meanwhite.zip, digits=2)` and $\sigma^2_{white} =$ `r round(varwhite.zip, digits=2)` for white respondents.

```{r zinb, include=FALSE, message = FALSE}
mod.zinb <- zeroinfl(resp ~ race | race,  dist = 'negbin', data = data)
mnull <- update(mod.zinb, . ~ 1)
#pchisq(2 * (logLik(mod.zinb) - logLik(mnull)), df = 3, lower.tail = FALSE)
sumzinb <- summary(mod.zinb)  
sumzinb$coefficients$count %>% 
  rbind(sumzinb$coefficients$zero) %>%
  kable(booktabs = T, 
        caption = "Zero-inflated Negative Binomial.") %>%
  group_rows("Count",1,3) %>%
  group_rows("Probability of zero",4,5) %>%
  kable_styling()
E2 <- resid(mod.zinb, type = "pearson")
N  <- nrow(data)
p  <- length(coef(mod.zinb))  
disp.zinb <- sum(E2^2) / (N - p)#1.06dispersion statistic gets really close to one!
aic.zinb <- AIC(mod.zinb)
intercept.zinb <- sumzinb$coefficients$count[1,1]
intercept.std.zinb <- sumzinb$coefficients$count[1,2]
race.zinb <- sumzinb$coefficients$count[2,1]
race.std.zinb <- sumzinb$coefficients$count[2,2]
intercept.zero.zinb <- sumzinb$coefficients$zero[1,1]
intercept.zero.std.zinb <- sumzinb$coefficients$zero[1,2]
race.zero.zinb <- sumzinb$coefficients$zero[2,1]
race.zero.std.zinb <- sumzinb$coefficients$zero[2,2]
lambdablack.zinb <- exp(intercept.zinb)
lambdawhite.zinb <- exp(intercept.zinb + race.zinb)
piblack.zinb <- exp(intercept.zero.zinb) /  (1 + exp(intercept.zero.zinb)) #binomial model
piwhite.zinb <- exp(intercept.zero.zinb + race.zero.zinb) / 
  (1 + exp(intercept.zero.zinb + race.zero.zinb)) #binomial model
meanblack.zinb <- (1 - piblack.zinb) * lambdablack.zinb
varblack.zinb <- (1 - piblack.zinb) * lambdablack.zinb * (1 + lambdablack.zinb * (piblack.zinb + 1/mod.zinb$theta))# do we need to use the log(theta) or theta?
meanwhite.zinb <- (1 - piwhite.zinb) * lambdawhite.zinb
varwhite.zinb <- (1 - piwhite.zinb) * lambdawhite.zinb * (1 + lambdawhite.zinb * (piwhite.zinb + 1/mod.zinb$theta))# do we need to use the log(theta) or theta?

#https://data.princeton.edu/wws509/notes/countmoments
```

For the zero-inflated negative binomial (zinb) model, the mean and variance are calculated as:

\vspace*{-5mm}
\begin{center}
$\mu_i = (1-\pi_i)\lambda_i$ and $\sigma^2_i = (1-\pi_i)\lambda_i(1 + \lambda_i(\pi_i + \alpha))$ with $i \in \{white, black\},$
\end{center}
\vspace*{-5mm}

where $\lambda$ is the average for the negative binomial process, $\pi$ the probability of zero, and $\alpha = \frac{1}{\theta}$ the overdispersion parameter. This yields $\mu_{black}=$ `r round(meanblack.zinb, digits=2)` and $\sigma^2_{black} =$ `r round(varblack.zinb, digits=2)` for black respondents, and $\mu_{white}=$ `r round(meanwhite.zinb, digits=2)` and $\sigma^2_{white} =$ `r round(varwhite.zinb, digits=2)` for white respondents.
It is interesting to see that $\theta$ is not significant; a zero-inflated Poisson where ($\theta$ = 1) might perform just as good.

# Conclusion {#CON}

Table \@ref(tab:comparemodels) shows the different models' estimate coefficients and Akaike Information Criterion (AIC). Estimated mean and variance for both races are also included in the table. 

All models agree that black respondents know significantly more homicide victims than white ones. The mean and variance for the number of homicide victims a black or white respondent knows is closest to the sample mean and variance in the zip model and the zinb model is a close second. The zip model also has the lowest AIC. 

Figure \@ref(fig:comparemodels3) compares the different models graphically. It is clear that the Poisson model performs the worst, especially for black respondents. The negative binomial is slightly better than the Poisson model for black respondents, but performs poorly for white ones. The zero-inflated models are the closest to the true data where zip seems to slightly outperform the zinb model - it almost entirely overlaps with the sample probabilities. 

Figure \@ref(fig:comparemodels3) shows the difference between the predicted and observed probabilities more clearly. Overall, the zip model comes closest to the sample probabilities.

\renewcommand{\arraystretch}{0.5}


# Appendix {-}


```{r comparemodels, include=TRUE, message = FALSE, fig.pos='H'}
comparison <- data.frame(data = unlist(c(NA, NA, NA, NA, NA, NA, 
                                sprintf("%.2f",summ[1,'mean']), summ[1,'variance'],
                                summ[2,'mean'], summ[2,'variance'], NA)),
                         poisson = c(sprintf("%.2f (%.2f) %s",intercept.poisson, 
                                             intercept.std.poisson, 
                                             stars.pval(sum.mod.poisson$coefficients[1,4])), 
                                     sprintf("%.2f (%.2f) %s",race.poisson, 
                                             race.std.poisson,
                                             stars.pval(sum.mod.poisson$coefficients[2,4])), 
                                     NA, NA, NA, NA,  
                                     round(meanblack.poisson,2), round(meanblack.poisson,2), 
                                     round(meanwhite.poisson,2), round(meanwhite.poisson,2), 
                                     round(aic.poisson,2)),
                         negbin = c(sprintf("%.2f (%.2f) %s",intercept.nb, 
                                            intercept.std.nb,
                                            stars.pval(sumnb$coefficients[1,4])), 
                                    sprintf("%.2f (%.2f) %s",race.nb, 
                                            race.std.poisson,
                                            stars.pval(sumnb$coefficients[2,4])), 
                                    sprintf("%.2f (%.2f)",round(mod.nb$theta,2),
                                            round(sumnb$SE.theta,2)), 
                                    NA, NA, NA, 
                                    round(meanblack.nb,2), round(varblack.nb,2), 
                                    round(meanwhite.nb,2), round(varwhite.nb,2), 
                                    round(aic.nb,2)),
                         quasil = c(sprintf("%.2f (%.2f) %s",intercept.quasip , 
                                            intercept.std.quasip,
                                            stars.pval(sum.quasip$coefficients[1,4])), 
                                    sprintf("%.2f (%.2f) %s",race.quasip, 
                                            race.std.quasip,
                                            stars.pval(sum.quasip$coefficients[2,4])), 
                                    NA, round(sum.quasip$dispersion,2), NA, NA,  
                                    round(meanblack.quasip,2), round(varblack.quasip,2), 
                                    round(meanwhite.quasip,2), round(varwhite.quasip,2), 
                                    round(aic.quasip,2)),
                         zip = c(sprintf("%.2f (%.2f) %s",intercept.zip , 
                                         intercept.std.zip,
                                         stars.pval(sumzip$coefficients$count[1,4])), 
                                 sprintf("%.2f (%.2f) %s", race.zip, 
                                         race.std.zip,
                                         stars.pval(sumzip$coefficients$count[2,4])), 
                                 NA, NA,
                                 sprintf("%.2f (%.2f) %s",intercept.zero.zip , 
                                         intercept.zero.std.zip, 
                                         stars.pval(sumzip$coefficients$zero[1,4])), 
                                 sprintf("%.2f (%.2f) %s",race.zero.zip, 
                                         race.zero.std.zip,
                                         stars.pval(sumzip$coefficients$zero[2,4])), 
                                 round(meanblack.zip,2), round(varblack.zip,2), 
                                 round(meanwhite.zip,2), round(varwhite.zip,2), round(aic.zip,2)),
                         zinb = c(sprintf("%.2f (%.2f) %s",intercept.zinb , 
                                          intercept.std.zinb,
                                          stars.pval(sumzinb$coefficients$count[1,4])), 
                                  sprintf("%.2f (%.2f) %s",race.zinb, 
                                          race.std.zinb,
                                          stars.pval(sumzinb$coefficients$count[2,4])), 
                                  sprintf("%.2f (%.2f)", round(mod.zinb$theta,2),
                                          round(exp(sumzinb$SE.logtheta),2)), 
                                  NA,
                                  sprintf("%.2f (%.2f) %s",intercept.zero.zinb , 
                                          intercept.zero.std.zinb,
                                          stars.pval(sumzinb$coefficients$zero[1,4])), 
                                  sprintf("%.2f (%.2f) %s",race.zero.zinb, 
                                          race.zero.std.zinb,
                                          stars.pval(sumzinb$coefficients$zero[2,4])), 
                                  round(meanblack.zinb,2), round(varblack.zinb,2), 
                                  round(meanwhite.zinb,2), round(varwhite.zinb,2), round(aic.zinb,2))) 
rownames(comparison) <- c('Intercept', 'racewhite', 'theta','phi','Intercept ', 'racewhite ', 
                          'mean black', 'variance black','mean white', 'variance white', 
                          'AIC')
comparison %>%
  kable(booktabs = T, 
        caption = sprintf("Comparison of all models with the data. For model parameters, standard deviations are listed between brackets, stars indicate siginifcance (%s).",attr(stars.pval(1),"legend")),
        row.names = TRUE,
        col.names = c('Data', 'Poisson', 'Negative binomial', 'Quasi likelihood',
                      'Zero-inflated  Poisson','Zero-Inflated Negative binomial')) %>%
  group_rows("Count",1,2) %>%
  group_rows("Probability of zero",5,6) %>%
  group_rows("Estimated mean and variance",7,10) %>%
  group_rows("Model performance",11,11) %>%
  column_spec(3:7, width = "2.1cm") %>%
  column_spec(2, width = "1cm") %>%
  kable_styling(font_size = 8)
```


```{r comparemodels2, include=TRUE, message = FALSE, warning = FALSE, fig.cap = "Graphical comparison of the sample probabilities and the predicted probabilities.", fig.height = 5, fig.width = 10}
data %>% group_by(race, resp) %>%
  summarize(n = n()) %>% 
  ungroup() %>% 
  group_by(race) %>%
  mutate(data = n/sum(n)) %>%
  complete(race, resp, fill = list(n = 0, perc = 0)) %>%
  distinct() %>%
  arrange(race, resp) %>%
  mutate(poisson = ifelse(race =="black",
                          exp(-1*meanblack.poisson)*meanblack.poisson^resp/factorial(resp),
                          exp(-1*meanwhite.poisson)*meanwhite.poisson^resp/factorial(resp)
         ),
         nb = ifelse(race == "black",
                     dnbinom(resp, size = mod.nb$theta, mu = meanblack.nb),
                     dnbinom(resp, size = mod.nb$theta, mu = meanblack.nb)),
         zip = ifelse(race =="black", 
                      ifelse(resp==0, piblack.zip + (1-piblack.zip) *
                               exp(-1*exp(intercept.zip)),
                             (1-piblack.zip) * exp(-1*exp(intercept.zip)) * 
                               exp(intercept.zip)^resp / factorial(resp)),
                      ifelse(resp==0, piwhite.zip + (1-piwhite.zip) * 
                               exp(-1*exp(intercept.zip + race.zip)),
                             (1-piwhite.zip) * exp(-1*exp(intercept.zip + race.zip)) * 
                               exp(intercept.zip + race.zip)^resp / factorial(resp))
                      ),
         zinb = ifelse(race =="black",
                       ifelse(resp==0, 
                              piblack.zinb + (1-piblack.zinb) *
                                dnbinom(0, size = mod.zinb$theta, 
                                        mu = exp(intercept.zinb)),
                              (1-piblack.zip) * dnbinom(resp, size = mod.zinb$theta, 
                                                        mu = exp(intercept.zinb))),
                       ifelse(resp == 0, 
                              piwhite.zip + (1 - piwhite.zip) * 
                                dnbinom(0, size = mod.zinb$theta, 
                                        mu = exp(intercept.zinb + race.zinb)),
                             (1 - piwhite.zip) * 
                               dnbinom(resp, size = mod.zinb$theta, 
                                       mu = exp(intercept.zinb + race.zinb))
                             )
                       )
         ) %>%
  pivot_longer(cols = 4:8, names_to = "model", values_to = "probability") -> A
A %>%
  ggplot() + 
  geom_line(aes(x = resp, y = probability, color = model)) +
  geom_point(aes(x = resp, y = probability, color = model), 
             fill = NA, shape = 21) +
  facet_grid(cols = vars(race)) + ylab("Probability") +
  theme_bw() + theme(legend.position = c(0.85, 0.65),
        legend.key = element_blank(),
        legend.background = element_blank()) +
  guides(color = guide_legend(override.aes = list(fill = NA))) +
  scale_color_manual(name = '',
                     breaks = c('data', 'nb', 'poisson', 'zip', 'zinb'),
                     labels = c('data', 'neg bin', 'poisson', 'zi poisson', 'zi negbin'),
                     values = c("#F8766D", "#BB9D00", "#00C0B8", "#00A5FF", "#E76BF3")) -> p1
```


```{r comparemodels3, include=TRUE, message = FALSE, warning = FALSE, fig.cap = "Top figures show the predicted probabilities and sample probabilities in the data. The bottom figures show the predicted probability minus the sample probability.", fig.height = 5, fig.width = 5, fig.pos='H'}
sampleprob <- A[A$model == "data", c('race', 'resp','probability')] %>%
  rename(prob = probability) %>%
  replace_na(list(prob = 0))
A %>% filter(model != "data") %>%
  left_join(sampleprob, by = c("resp" = "resp", "race" = "race")) %>%
  mutate(
    diff = probability - prob
  ) %>%
  ggplot() + geom_line(aes(x = resp, y = diff, color = model)) +
  geom_point(aes(x = resp, y = diff, color = model), fill = NA, shape = 21) +
  facet_grid(cols = vars(race)) +
  theme_bw() +
  theme(legend.position = "none") +
  ylab("Difference in probability") +
  scale_color_manual(name = '',
                     breaks = c('nb', 'poisson', 'zip', 'zinb'),
                     labels = c('neg bin', 'poisson', 'zi poisson', 'zi negbin'),
                     values = c("#BB9D00", "#00C0B8", "#00A5FF", "#E76BF3")) -> p2
plot_grid(p1,p2, nrow = 2)
```
